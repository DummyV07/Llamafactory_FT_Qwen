{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据集制作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "已将 xingfa.docx 转换为 output.txt\n"
     ]
    }
   ],
   "source": [
    "# docx --> txt\n",
    "from docx import Document\n",
    "\n",
    "# 读取 .docx 文件\n",
    "def docx_to_text(docx_file, txt_file):\n",
    "    doc = Document(docx_file)\n",
    "    with open(txt_file, 'w', encoding='utf-8') as f:\n",
    "        for para in doc.paragraphs:\n",
    "            f.write(para.text + '\\n')\n",
    "\n",
    "# 调用函数转换\n",
    "docx_file = 'xingfa.docx'  # 你的 .docx 文件路径\n",
    "txt_file = 'output.txt'    # 转换后的 .txt 文件路径\n",
    "# docx_to_text(docx_file, txt_file)\n",
    "\n",
    "print(f'已将 {docx_file} 转换为 {txt_file}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正克隆到 'bert-base-chinese'...\n",
      "remote: Enumerating objects: 20, done.\u001b[K\n",
      "remote: Counting objects: 100% (20/20), done.\u001b[K\n",
      "remote: Compressing objects: 100% (17/17), done.\u001b[K\n",
      "remote: Total 20 (delta 3), reused 0 (delta 0), pack-reused 0\u001b[K\n",
      "接收对象中: 100% (20/20), 153.57 KiB | 1.14 MiB/s, 完成.\n",
      "处理 delta 中: 100% (3/3), 完成.\n"
     ]
    }
   ],
   "source": [
    "# 使用bert进行chunk切分\n",
    "! git clone https://www.modelscope.cn/tiansz/bert-base-chinese.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Incorrect path_or_model_id: '/mnt/workspace/dataset/bert-base-chinese'. Please provide either the path to a local folder or the repo_id of a model on the Hub.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHFValidationError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.8/site-packages/transformers/utils/hub.py:402\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    400\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    401\u001b[0m     \u001b[38;5;66;03m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[0;32m--> 402\u001b[0m     resolved_file \u001b[38;5;241m=\u001b[39m \u001b[43mhf_hub_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    403\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    404\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    405\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    406\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    407\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    408\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    409\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    410\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    411\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    412\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    413\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    414\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    415\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    416\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m GatedRepoError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:101\u001b[0m, in \u001b[0;36m_deprecate_arguments.<locals>._inner_deprecate_positional_args.<locals>.inner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    100\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m)\n\u001b[0;32m--> 101\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.8/site-packages/huggingface_hub/utils/_validators.py:106\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m arg_name \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrepo_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrom_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto_id\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m--> 106\u001b[0m     \u001b[43mvalidate_repo_id\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg_value\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m arg_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m arg_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.8/site-packages/huggingface_hub/utils/_validators.py:154\u001b[0m, in \u001b[0;36mvalidate_repo_id\u001b[0;34m(repo_id)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m repo_id\u001b[38;5;241m.\u001b[39mcount(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 154\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HFValidationError(\n\u001b[1;32m    155\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRepo id must be in the form \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrepo_name\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m or \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnamespace/repo_name\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    156\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrepo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. Use `repo_type` argument if needed.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    157\u001b[0m     )\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m REPO_ID_REGEX\u001b[38;5;241m.\u001b[39mmatch(repo_id):\n",
      "\u001b[0;31mHFValidationError\u001b[0m: Repo id must be in the form 'repo_name' or 'namespace/repo_name': '/mnt/workspace/dataset/bert-base-chinese'. Use `repo_type` argument if needed.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 128\u001b[0m\n\u001b[1;32m    125\u001b[0m similarity_threshold \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.5\u001b[39m  \u001b[38;5;66;03m# 可根据需要调整\u001b[39;00m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;66;03m# 分割长文本\u001b[39;00m\n\u001b[0;32m--> 128\u001b[0m text_chunks \u001b[38;5;241m=\u001b[39m \u001b[43msplit_text_by_semantic\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlong_text\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msimilarity_threshold\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;66;03m# 保存分割后的文本块到指定目录\u001b[39;00m\n\u001b[1;32m    131\u001b[0m save_chunks_to_files(text_chunks, output_dir)\n",
      "Cell \u001b[0;32mIn[3], line 42\u001b[0m, in \u001b[0;36msplit_text_by_semantic\u001b[0;34m(text, max_length, similarity_threshold)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;124;03m基于语义相似度对文本进行分块\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;124;03mlist: 分割后的文本块列表\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m# 加载BERT模型和分词器\u001b[39;00m\n\u001b[0;32m---> 42\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mBertTokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/mnt/workspace/dataset/bert-base-chinese\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m model \u001b[38;5;241m=\u001b[39m BertModel\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/mnt/workspace/dataset/bert-base-chinese\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     44\u001b[0m model\u001b[38;5;241m.\u001b[39meval()  \u001b[38;5;66;03m# 设置模型为评估模式\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2210\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, trust_remote_code, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   2207\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokenizer_file\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m vocab_files:\n\u001b[1;32m   2208\u001b[0m     \u001b[38;5;66;03m# Try to get the tokenizer config to see if there are versioned tokenizer files.\u001b[39;00m\n\u001b[1;32m   2209\u001b[0m     fast_tokenizer_file \u001b[38;5;241m=\u001b[39m FULL_TOKENIZER_FILE\n\u001b[0;32m-> 2210\u001b[0m     resolved_config_file \u001b[38;5;241m=\u001b[39m \u001b[43mcached_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2211\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2212\u001b[0m \u001b[43m        \u001b[49m\u001b[43mTOKENIZER_CONFIG_FILE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2213\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2214\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2215\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2216\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2217\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2218\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2219\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2220\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2221\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2222\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_raise_exceptions_for_gated_repo\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   2223\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_raise_exceptions_for_missing_entries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   2224\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_raise_exceptions_for_connection_errors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   2225\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2226\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2227\u001b[0m     commit_hash \u001b[38;5;241m=\u001b[39m extract_commit_hash(resolved_config_file, commit_hash)\n\u001b[1;32m   2228\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m resolved_config_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.8/site-packages/transformers/utils/hub.py:466\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    464\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThere was a specific connection error when trying to load \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00merr\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    465\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HFValidationError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 466\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[1;32m    467\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIncorrect path_or_model_id: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. Please provide either the path to a local folder or the repo_id of a model on the Hub.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    468\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    469\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resolved_file\n",
      "\u001b[0;31mOSError\u001b[0m: Incorrect path_or_model_id: '/mnt/workspace/dataset/bert-base-chinese'. Please provide either the path to a local folder or the repo_id of a model on the Hub."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import re\n",
    "import os\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "\n",
    "def get_sentence_embedding(sentence, model, tokenizer):\n",
    "    \"\"\"\n",
    "    获取句子的嵌入表示\n",
    "\n",
    "    参数:\n",
    "    sentence (str): 输入句子\n",
    "    model (BertModel): 预训练的BERT模型\n",
    "    tokenizer (BertTokenizer): BERT分词器\n",
    "\n",
    "    返回:\n",
    "    numpy.ndarray: 句子的嵌入向量\n",
    "    \"\"\"\n",
    "    # 使用分词器处理输入句子，并转换为模型输入格式\n",
    "    inputs = tokenizer(sentence, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "    # 使用模型获取输出，不计算梯度\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    # 返回最后一层隐藏状态的平均值作为句子嵌入\n",
    "    return outputs.last_hidden_state.mean(dim=1).squeeze().numpy()\n",
    "\n",
    "\n",
    "def split_text_by_semantic(text, max_length, similarity_threshold=0.5):\n",
    "    \"\"\"\n",
    "    基于语义相似度对文本进行分块\n",
    "\n",
    "    参数:\n",
    "    text (str): 输入的长文本\n",
    "    max_length (int): 每个文本块的最大长度（以BERT分词器的token为单位）\n",
    "    similarity_threshold (float): 语义相似度阈值，默认为0.5\n",
    "\n",
    "    返回:\n",
    "    list: 分割后的文本块列表\n",
    "    \"\"\"\n",
    "    # 加载BERT模型和分词器\n",
    "    tokenizer = BertTokenizer.from_pretrained('./bert-base-chinese')\n",
    "    model = BertModel.from_pretrained('./bert-base-chinese')\n",
    "    model.eval()  # 设置模型为评估模式\n",
    "\n",
    "    # 按句子分割文本（使用常见的中文标点符号）\n",
    "    sentences = re.split(r'(。|！|？|；)', text)\n",
    "    # 重新组合句子和标点\n",
    "    sentences = [s + p for s, p in zip(sentences[::2], sentences[1::2]) if s]\n",
    "\n",
    "    chunks = []\n",
    "    current_chunk = sentences[0]\n",
    "    # 获取当前chunk的嵌入表示\n",
    "    current_embedding = get_sentence_embedding(current_chunk, model, tokenizer)\n",
    "\n",
    "    for sentence in sentences[1:]:\n",
    "        # 获取当前句子的嵌入表示\n",
    "        sentence_embedding = get_sentence_embedding(sentence, model, tokenizer)\n",
    "        # 计算当前chunk和当前句子的余弦相似度\n",
    "        similarity = 1 - cosine(current_embedding, sentence_embedding)\n",
    "\n",
    "        # 如果相似度高于阈值且合并后不超过最大长度，则合并\n",
    "        if similarity > similarity_threshold and len(tokenizer.tokenize(current_chunk + sentence)) <= max_length:\n",
    "            current_chunk += sentence\n",
    "            # 更新当前chunk的嵌入表示\n",
    "            current_embedding = (current_embedding + sentence_embedding) / 2\n",
    "        else:\n",
    "            # 否则，保存当前chunk并开始新的chunk\n",
    "            chunks.append(current_chunk)\n",
    "            current_chunk = sentence\n",
    "            current_embedding = sentence_embedding\n",
    "\n",
    "    # 添加最后一个chunk\n",
    "    if current_chunk:\n",
    "        chunks.append(current_chunk)\n",
    "\n",
    "    return chunks\n",
    "\n",
    "\n",
    "def read_text_file(file_path):\n",
    "    \"\"\"\n",
    "    读取文本文件\n",
    "\n",
    "    参数:\n",
    "    file_path (str): 文件路径\n",
    "\n",
    "    返回:\n",
    "    str: 文件内容\n",
    "    \"\"\"\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        return file.read()\n",
    "\n",
    "\n",
    "def save_chunks_to_files(chunks, output_dir):\n",
    "    \"\"\"\n",
    "    将分割后的文本块保存到文件\n",
    "\n",
    "    参数:\n",
    "    chunks (list): 文本块列表\n",
    "    output_dir (str): 输出目录路径\n",
    "    \"\"\"\n",
    "    # 如果输出目录不存在，则创建\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    # 将每个文本块保存为单独的文件\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        chunk_file_path = os.path.join(output_dir, f\"chunk_{i + 1}.txt\")\n",
    "        with open(chunk_file_path, 'w', encoding='utf-8') as file:\n",
    "            file.write(chunk)\n",
    "        print(f\"已保存第 {i + 1} 个文本块到 {chunk_file_path}\")\n",
    "\n",
    "\n",
    "# 主程序\n",
    "\n",
    "# 设置输入和输出路径\n",
    "input_file_path = './output.txt'  # 替换为你的长文本文件路径\n",
    "output_dir = './saveChunk/'  # 替换为你希望保存文本块的目录路径\n",
    "\n",
    "# 读取长文本\n",
    "long_text = read_text_file(input_file_path)\n",
    "\n",
    "# 设置每个文本块的最大分词数量和相似度阈值\n",
    "max_length = 2048  # 可根据需要调整\n",
    "similarity_threshold = 0.5  # 可根据需要调整\n",
    "\n",
    "# 分割长文本\n",
    "text_chunks = split_text_by_semantic(long_text, max_length, similarity_threshold)\n",
    "\n",
    "# 保存分割后的文本块到指定目录\n",
    "save_chunks_to_files(text_chunks, output_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用API进行数据集生成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import time\n",
    "import re\n",
    "from typing import List, Dict\n",
    "from openai import OpenAI\n",
    "import logging\n",
    "import backoff\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "# 设置日志\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# 从环境变量中获取 API 密钥\n",
    "api_key = \"sk-76ea27b0a134466a9afca900b45e849c\"\n",
    "\n",
    "# 初始化 OpenAI 客户端\n",
    "client = OpenAI(base_url=\"https://dashscope.aliyuncs.com/compatible-mode/v1\", api_key=api_key)\n",
    "\n",
    "def read_file(file_path: str) -> str:\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        return file.read()\n",
    "\n",
    "@backoff.on_exception(backoff.expo, Exception, max_tries=3)\n",
    "def generate_single_entry(text: str) -> Dict:\n",
    "    prompt = f\"\"\"\n",
    "    基于以下文本，生成1个用于指令数据集的高质量条目。条目应该直接关联到给定的文本内容，提出相关的问题或任务。\n",
    "    请确保生成多样化的指令类型，例如：\n",
    "    - 分析类：\"分析...\"\n",
    "    - 比较类：\"比较...\"\n",
    "    - 解释类：\"解释...\"\n",
    "    - 评价类：\"评价...\"\n",
    "    - 问答类：\"为什么...\"\n",
    "\n",
    "    文本内容：\n",
    "    {text}\n",
    "\n",
    "    请以下面的格式生成条目，确保所有字段都有适当的内容：\n",
    "    {{\n",
    "        \"instruction\": \"使用上述多样化的指令类型之一，提出一个具体的、与文本相关的问题或任务\",\n",
    "        \"input\": \"如果需要额外的上下文信息，请在这里提供，否则留空\",\n",
    "        \"output\": \"对instruction的详细回答或任务的完成结果\"\n",
    "    }}\n",
    "    确保所有生成的内容都与给定的文本直接相关，生成的是有效的JSON格式，并且内容高质量、准确、详细。\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"qwen-plus-0919\",  # 尝试使用自己选择的 模型\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            temperature=0.7,  # 增加温度以提高多样性\n",
    "            max_tokens=4098\n",
    "        )\n",
    "        logger.info(f\"API 响应: {response.choices[0].message.content}\")\n",
    "\n",
    "        json_match = re.search(r'\\{.*\\}', response.choices[0].message.content, re.DOTALL)\n",
    "        if json_match:\n",
    "            entry = json.loads(json_match.group())\n",
    "            required_keys = ['instruction', 'input', 'output']\n",
    "            if isinstance(entry, dict) and all(key in entry for key in required_keys):\n",
    "                # 根据 input 是否为空来设置 text 字段\n",
    "                if entry['input'].strip():\n",
    "                    entry[\n",
    "                        'text'] = f\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.### Instruction: {entry['instruction']}\\n### Input: {entry['input']}\\n### Response: {entry['output']}\"\n",
    "                else:\n",
    "                    entry[\n",
    "                        'text'] = f\"Below is an instruction that describes a task. Write a response that appropriately completes the request.### Instruction: {entry['instruction']}\\n### Input: {entry['input']}\\n### Response: {entry['output']}\"\n",
    "\n",
    "                logger.info(\"成功生成完整条目\")\n",
    "                return entry\n",
    "            else:\n",
    "                logger.warning(\"JSON 解析成功，但缺少必要字段\")\n",
    "                return {}\n",
    "        else:\n",
    "            logger.error(\"无法从API响应中提取有效的JSON\")\n",
    "            return {}\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"生成条目时发生错误: {str(e)}\")\n",
    "        return {}\n",
    "\n",
    "def process_file(file_path: str, entries_per_file: int) -> List[Dict]:\n",
    "    dataset = []\n",
    "    try:\n",
    "        text = read_file(file_path)\n",
    "        for j in range(entries_per_file):\n",
    "            logger.info(f\"  生成第 {j + 1}/{entries_per_file} 个条目\")\n",
    "            entry = generate_single_entry(text)\n",
    "            if entry and all(key in entry for key in ['instruction', 'input', 'output', 'text']):\n",
    "                dataset.append(entry)\n",
    "                logger.info(f\"  成功生成 1 个完整条目\")\n",
    "            else:\n",
    "                logger.warning(f\"  跳过不完整的条目\")\n",
    "            time.sleep(2)  # 在请求之间增加延迟到2秒\n",
    "    except Exception as e:\n",
    "        logger.error(f\"处理文件 {file_path} 时发生未知异常: {str(e)}\")\n",
    "    return dataset\n",
    "\n",
    "def generate_dataset(folder_path: str, entries_per_file: int = 2) -> List[Dict]:\n",
    "    dataset = []\n",
    "    files = [os.path.join(folder_path, filename) for filename in os.listdir(folder_path) if filename.endswith(\".txt\")]\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=4) as executor:  # 调整 max_workers 数量以适应你的硬件资源\n",
    "        futures = [executor.submit(process_file, file_path, entries_per_file) for file_path in files]\n",
    "        for future in as_completed(futures):\n",
    "            try:\n",
    "                dataset.extend(future.result())\n",
    "            except Exception as e:\n",
    "                logger.error(f\"处理未来任务时发生未知异常: {str(e)}\")\n",
    "\n",
    "    return dataset\n",
    "\n",
    "def save_dataset_as_parquet(dataset: List[Dict], output_file: str):\n",
    "    schema = pa.schema([\n",
    "        ('instruction', pa.string()),\n",
    "        ('input', pa.string()),\n",
    "        ('output', pa.string()),\n",
    "        ('text', pa.string())\n",
    "    ])\n",
    "\n",
    "    arrays = [\n",
    "        pa.array([entry['instruction'] for entry in dataset]),\n",
    "        pa.array([entry['input'] for entry in dataset]),\n",
    "        pa.array([entry['output'] for entry in dataset]),\n",
    "        pa.array([entry['text'] for entry in dataset])\n",
    "    ]\n",
    "\n",
    "    table = pa.Table.from_arrays(arrays, schema=schema)\n",
    "    pq.write_table(table, output_file)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input_folder = \"./saveChunk\"  # 指定输入文件夹路径\n",
    "    output_file = \"instruction_dataset.parquet\"\n",
    "\n",
    "    logger.info(\"开始生成数据集\")\n",
    "    dataset = generate_dataset(input_folder, entries_per_file=10)\n",
    "    save_dataset_as_parquet(dataset, output_file)\n",
    "    logger.info(f\"数据集已生成并保存到 {output_file}\")\n",
    "    logger.info(f\"共生成 {len(dataset)} 个有效条目\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
